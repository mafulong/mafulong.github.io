---

layout: post
category: Mq
title: kafka原理1-基础架构
tags: Mq
recent_update: true
---

## KafkaServer



使用消息队列的主要好处有：

- 解耦
- 峰值处理能力、异步
- 持久化
- 顺序保证
- 扩展性

## 1. 架构

### 1.1 概念

**Broker：**

Kafka集群中的一台服务器。

**Topic：**

主题。消息的分类，是逻辑上的概念，实际以partition的形式存放在各Broker服务器上。

**Partition：**

-   分区。是物理上的概念，组成Topic的单位。

**Replica：**

副本。是具体的分区，比Partition更具体的物理概念，一个真实的目录，存放在一台Broker服务器上。在代码中又分为leader和follower。

**Producer：**

-   生产者。负责发送消息到Broker。

**Consumer：**

-   消费者。负责订阅Topic并拉取消息。

**CousumerGroup：**

消费者组。每条消息只被组内成员消费一次。

### 1.2 逻辑架构

![image](https://cdn.jsdelivr.net/gh/mafulong/mdPic@master/images/60af40353a110cbc9948067b4502a75d.png)



#### 生产者



<img src="https://cdn.jsdelivr.net/gh/mafulong/mdPic@vv5/v5/202502031652488.png" alt="img" style="zoom:50%;" />

整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和 Sender 线程（发送线程）。

在主线程中由 KafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（RecordAccumulator，也称为消息收集器）中。Sender 线程负责从 RecordAccumulator 中获取消息并将其发送到 Kafka 中。

RecordAccumulator 主要用来缓存消息以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能。

主线程中发送过来的消息都会被追加到 RecordAccumulator 的某个双端队列（Deque）中，在 RecordAccumulator 的内部为每个分区都维护了一个双端队列。

**消息写入缓存时，追加到双端队列的尾部；Sender 读取消息时，从双端队列的头部读取。**

Sender 从 RecordAccumulator 中获取缓存的消息之后，会进一步将原本<分区, Deque< ProducerBatch>> 的保存形式转变成 <Node, List< ProducerBatch> 的形式，其中 Node 表示 Kafka 集群的 broker 节点。

KafkaProducer 要将此消息追加到指定主题的某个分区所对应的 leader 副本之前，首先需要知道主题的分区数量，然后经过计算得出（或者直接指定）目标分区，之后 KafkaProducer 需要知道目标分区的 leader 副本所在的 broker 节点的地址、端口等信息才能建立连接，最终才能将消息发送到 Kafka。



**生产者拦截器既可以用来在消息发送前做一些准备工作，比如按照某个规则过滤不符合要求的消息、修改消息的内容等，也可以用来在发送回调逻辑前做一些定制化的需求，比如统计类工作。**



**生产者需要用序列化器（Serializer）把对象转换成字节数组才能通过网络发送给 Kafka。而在对侧，消费者需要用反序列化器（Deserializer）把从 Kafka 中收到的字节数组转换成相应的对象。**



**消息经过序列化之后就需要确定它发往的分区，如果消息 ProducerRecord 中指定了 partition 字段，那么就不需要分区器的作用，因为 partition 代表的就是所要发往的分区号。**

**如果消息 ProducerRecord 中没有指定 partition 字段，那么就需要依赖分区器，根据 key 这个字段来计算 partition 的值。分区器的作用就是为消息分配分区。**

#### Broker

在Kafka的架构中，会有很多客户端向Broker端发送请求，Kafka 的 Broker 端有个 SocketServer 组件，用来和客户端建立连接，然后通过Acceptor线程来进行请求的分发，由于Acceptor不涉及具体的逻辑处理，非常得轻量级，因此有很高的吞吐量。

接着Acceptor 线程采用轮询的方式将入站请求公平地发到所有网络线程中，网络线程池默认大小是 3个，表示每台 Broker 启动时会创建 3 个网络线程，专门处理客户端发送的请求，可以通过Broker 端参数 num.network.threads来进行修改。

**当网络线程拿到请求后，会将请求放入到一个共享请求队列中。Broker 端还有个 IO 线程池，负责从该队列中取出请求，执行真正的处理。如果是 PRODUCE 生产请求，则将消息写入到底层的磁盘日志中；如果是 FETCH 请求，则从磁盘或页缓存中读取消息。**



#### 控制器 Controller

在 Kafka 集群中会有一个或多个 broker，其中有一个 broker 会被选举为控制器（Kafka Controller），它负责管理整个集群中所有分区和副本的状态。

Broker 在启动时，会尝试去 ZooKeeper 中创建 /controller 节点。Kafka 当前选举控制器的规则是：**第一个成功创建 /controller 节点的 Broker 会被指定为控制器。** Kafka集群依赖Zookeeper，Zookeeper的数据模型是一棵树，kafka的组件把回调函数注册到zk树节点下，在节点发生变更时，zk通过回调通知kafka。



当 Partition 的 Leader 宕机时，Controller 会从 ISR 中 **选择最新同步的副本** 作为新的 Leader，并更新 ZooKeeper。



在ZooKeeper中的 /controller_epoch 节点中存放的是一个整型的 controller_epoch 值。controller_epoch 用于记录控制器发生变更的次数，即记录当前的控制器是第几代控制器，我们也可以称之为“控制器的纪元”。

controller_epoch 的初始值为1，即集群中第一个控制器的纪元为1，当控制器发生变更时，每选出一个新的控制器就将该字段值加1。**Kafka 通过 controller_epoch 来保证控制器的唯一性，进而保证相关操作的一致性。**每个和控制器交互的请求都会携带 controller_epoch 这个字段，如果请求的 controller_epoch 值小于内存中的 controller_epoch 值，则认为这个请求是向已经过期的控制器所发送的请求，那么这个请求会被认定为无效的请求。

当运行中的控制器突然宕机或意外终止时，Kafka 能够快速地感知到，并立即启用备用控制器来代替之前失败的控制器。这个过程就被称为 Failover，该过程是自动完成的，无需你手动干预。



-  在Kafka引入KRaft新内部功能后，对Zookeeper的依赖将会被取消。**在 KRaft 中，一部分 broker 被指定为控制器，这些控制器提供过去由 ZooKeeper 提供的共识服务**。所有集群元数据都将存储在 Kafka 主题中并在内部进行管理。

  现在controller就会多个了，但其中一个是主的，通过raft选举。原来数据存在在zk上，现在直接存在controller上。

  

  控制器是做什么的？

  

  

- zookeeper选举

- 保存的数据，都在zookeeper上

  <img src="https://cdn.jsdelivr.net/gh/mafulong/mdPic@vv3/v3/20210523170815.png" alt="image-20210523170815843" style="zoom:50%;" />



#### 消费者 Consumer

在Kafka中，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者。每个消费者只能消费所分配到的分区中的消息。**而每一个分区只能被一个消费组中的一个消费者所消费。**

消费者与消费组这种模型可以让整体的消费能力具备横向伸缩性，我们可以增加（或减少）消费者的个数来提高（或降低）整体的消费能力。对于分区数固定的情况，一味地增加消费者并不会让消费能力一直得到提升，如果消费者过多，出现了**消费者的个数大于分区个数**的情况，就会有消费者分配不到任何分区。

Kafka 定期自动删除过期位移的条件就是，组要处于 Empty 状态 (组内没有任何成员)。因此，如果你的消费者组停掉了很长时间（超过 7 天），那么 Kafka 很可能就把该组的位移数据删除了。



#### 组协调器（GroupCoordinator）

GroupCoordinator 是 Kafka 服务端中用于管理消费组的组件。协调器最重要的职责就是负责执行消费者再均衡的操作。

当组内成员加入组时，它会向协调器发送 JoinGroup 请求。在该请求中，每个成员都要将自己订阅的主题上报，这样协调器就能收集到所有成员的订阅信息。



### 1.3 物理架构

#### 1.3.1 集群物理架构

*|*![image](https://cdn.jsdelivr.net/gh/mafulong/mdPic@master/images/0993d208eeb5dfe12a9e6f132e9d0eef.png)

如上图，本集群有：

  - 4台broker。broker_1 到 broker_4
  - 2个topic。topic_A和topic_B
  - topic_A的分区数是4。topicA_0 到 topic_3
  - topic_A的副本因子是2。例如topicA_0这个分区，有两个副本，分别分布在broker_1和broker_3两台机器上。

#### 1.3.2 一个副本目录结构

上图中的topicA-0是一个partition，准确的说应该称之为一个副本，即Replica。它是broker_1服务器上的一个日志目录，其内部由多个segment文件组成。topicA-0目录结构：

*|*![image](https://cdn.jsdelivr.net/gh/mafulong/mdPic@master/images/15917459e5a10f3a6839bc535f0afbc6.png)

***.log文件**：日志文件

***.index文件**：稀疏索引文件

***.timeindex文件**：时间戳索引文件。根据时间戳快速定位消息所在位置。（Kafka API offsetsForTimes方法所使用）

index文件与log文件关系:

*|*![image](https://cdn.jsdelivr.net/gh/mafulong/mdPic@master/images/42a9581fb80035875abcaff0e6902542.png)

### 1.4 代码架构

#### 1.4.1 线程池

KafkaServer在启动时，初始化KafkaRequestHandlerPool线程池

**KafkaRequestHandlerPool（线程池）由KafkaRequestHandler组成**

KafkaRequestHandler调用KafkaApis处理request

下图：Kafka处理request的线程池

*|*![image](https://cdn.jsdelivr.net/gh/mafulong/mdPic@master/images/83fbd9155e52d987bb704a028f719772.png)

kafkaServer接收的请求，包括来自客户端的请求和来自其他server的请求。客户端请求例如producer发送消息请求、consumer消费消息请求。其他broker请求例如副本同步日志请求。请求类型有21个枚举值。

所有的请求最终会收敛到Broker服务器上的KafkaApis.handle方法，如下图：

*|*![image](https://cdn.jsdelivr.net/gh/mafulong/mdPic@master/images/9fb206ff88bb93e7d92cf7dced0e540b.png)

#### 1.4.2 处理request的组件

有3个重要组件处理上图21种请求，它们分别是：

- **ReplicaManager**

  - 把Produce日志写入磁盘
  - 如果副本是follower，启动副本同步线程，发送fetch请求
  - 如果副本是leader，处理来自副本的fetch请求
- **Coordinator**

  - 管理Consumer的balance

- **KafkaController**

  - Broker 的上线、下线
  - 新建 topic 或已有 topic 的扩容，topic 删除
  - 处理replica的分配、迁移、leader 选举、leader 切换

下图：处理request请求

*|*![image](https://cdn.jsdelivr.net/gh/mafulong/mdPic@master/images/d85330c319388dddad5de8b90e743c9a.png)









## 2. 可用性、可靠性保障

### 2.1 集群高可用性

在应对单点故障时，kafka仍然能够对外提供服务，主要通过以下特性保证：

- 分布式集群模式，多台服务器，分散单服务器压力。
- 一个topic拆分成多partition分区
- 一个partition建立n个副本，分为leader和follower，保持同步。leader一旦宕机从follower选举出新的leader提供读写日志服务。

### **2.2 消息可靠性与一致性**

面对故障时是否仍然保持最终一致。

- producer.ACKS = [ -1, 0, 1] 保证日志生产的准确性。
- HW和LEO机制。保证日志同步的准确性。

#### 2.2.1 producer.ACKs

**ACKs=1**

- producer发送日志，只要Leader写入成功，则返回producer成功

**ACKs=0** 

- producer发送日志，不需要等待leader返回成功
- 传输效率最高，可靠性最差

**ACKs=-1** 

- producer发送日志，Leader需要等待ISR中所有的Replica同步完成后，才返回给客户端成功
- 可靠性最高，效率最差。集群的瓶颈卡在了最差的那台机器

#### 2.2.2 HW和LEO

**LEO**  

- LogEndOffset，日志末端偏移量
- 每个Replica最后一条log所在的位置

**HW**  

- HighWaterMark，高水位
- HW = min( ISR.LEO )
- 已经被ISR完成同步的消息的位置
- Consumer最多只能消费到HW所在的位置
- 对于内部Replica的同步消息请求，没有HW的限制

如下图，HW与LEO位置示意：

![image](https://cdn.jsdelivr.net/gh/mafulong/mdPic@master/images/b9466c871eb1de0157d41377cd06d94a.png)

**HW与LEO更新过程：**

![image](https://cdn.jsdelivr.net/gh/mafulong/mdPic@master/images/d64eb94018321663dfe38c6ea278bbde.png)

**HW与LEO更新过程（详细）：**

HW与LEO存在于每一个副本，并不仅仅存在于leader。

leader中维护了两套LEO，一套是自己的，另一套是follower的。

假设目前消息队列为空，follower启动的同步消息线程，不会获取到任何消息，也不会更新HW和LEO

*|*![image](https://cdn.jsdelivr.net/gh/mafulong/mdPic@master/images/ec19a2cae1dc96baef8fe646f758b97a.png)

此时，Producer给leader发送了一条日志

1. leader的LEO + 1
2. leader尝试更新HW，HW = min(LEO)，仍然是0

*|*![image](https://cdn.jsdelivr.net/gh/mafulong/mdPic@master/images/219fab4d24791360c434d96ad75c71e8.png)

follower发送fetch请求：

*|*![image](https://cdn.jsdelivr.net/gh/mafulong/mdPic@master/images/65578cc63bfff21a8529caed7fd904ff.png)

1. req的offset参数是0，表示从第0个消息开始fetch
2. leader更新remote LEO=0，这是因为follower request的offset是0
1. leader尝试更新HW，HW = min(LEO)，仍然是0
1. leader把数据和此时的leader HW返回给follower
1. follower接收到respsonse，更新LEO=1，更新HW仍然是0



follower发送第二轮fetch请求:

![image](https://cdn.jsdelivr.net/gh/mafulong/mdPic@master/images/a730c7a7e7b6ea34fa9d3a71e1911f57.png)

1. req的offset参数是1，表示请求同步第一个消息
2. leader更新remote LEO=1，因为follower request的offset是1
3. leader尝试更新HW，HW=min(LEO)，**此时更新HW=1**
4. leader把数据和此时的leader HW返回给follower
5. follower接收到respsonse，更新HW，**此时更新HW=1**

至此，producer生产的消息已经保存到kafka的各个副本上了，Consumer已经可以消费到HW位置了。

一个消息从写入kafka到完成更新HW，需要follower发送两轮fetch请求。 第一轮fetch是follow告诉leader自己的leo, 以及更新offset以及leo.  第二轮fetch是告诉producer自己最新的leo用于leader更新hw.





对于ack设置为all的producer的一条写请求，leader会等到ISRs中的所有follower都拉取到此条日志后才会更新自己的HW，同时回复给producer写成功。**这里有一个很微妙的点在于，不同于一般的leader通知follower进行日志写入的模式，kafka中是由follower主动拉取leader的日志进行同步的。**

**这个类似于两阶段提交的模式其实很常见，比如在raft中，leader拿到多数节点的回复后并不会立马把结果再告知follower，而是等到客户端下次请求或是心跳的时候顺带将commit信息告知follower，这样leader只需要一轮交互就可以返回给客户端写成功。**



## Rebalance

**消费再均衡（Rebalancing）** 的原理是通过 Kafka 的消费组协调机制，在消费组内的消费者发生变化时（例如加入、退出、宕机等），自动调整每个消费者所消费的分区，以保证负载的平衡和数据的均匀消费。再均衡的过程由 **消费者协调器（Consumer Coordinator）** 和 **消费组协调器（Group Coordinator）** 来共同完成。



**组协调器（GroupCoordinator）**

GroupCoordinator 是 Kafka 服务端中用于管理消费组的组件。协调器最重要的职责就是负责执行消费者再均衡的操作。

当组内成员加入组时，它会向协调器发送 JoinGroup 请求。在该请求中，每个成员都要将自己订阅的主题上报，这样协调器就能收集到所有成员的订阅信息。

**GroupCoordinator 的分配**：**每个消费组都只有一个特定的 GroupCoordinator。**Kafka 并不会全局选举一个固定的 GroupCoordinator，而是根据消费组的名称和 Kafka 集群中 broker 的负载情况来动态选择一个 broker 来作为 GroupCoordinator。

**其实就是简单的hash名称分配一个broker。**



**Rebalance 过程对 Consumer Group 消费过程有极大的影响。在 Rebalance 过程中，所有 Consumer 实例都会停止消费，等待 Rebalance 完成。这是 Rebalance 为人诟病的一个方面。** 过程可能几个小时。

弊端：

1. 在再均衡发生期间，消费组内的消费者是无法读取消息的。
2. Rebalance 很慢。如果一个消费者组里面有几百个 Consumer 实例，Rebalance 一次要几个小时。
3. 在进行再均衡的时候消，费者当前的状态也会丢失。比如消费者消费完某个分区中的一部分消息时还没有来得及提交消费位移就发生了再均衡操作，之后这个分区又被分配给了消费组内的另一个消费者，原来被消费完的那部分消息又被重新消费一遍，也就是发生了重复消费。



**Rebalance 发生的时机有三个：**

1. 组成员数量发生变化。 当 Consumer Group 完成 Rebalance 之后，每个 Consumer 实例都会定期地向 Coordinator 发送心跳请求，表明它还存活着。如果某个消费者由于网络问题、GC 等原因未能发送心跳，GroupCoordinator 会认为该消费者下线并触发再均衡。**可以通过修改心跳请求的频率和超时时间来解决。**
2. 订阅主题数量发生变化
3. 订阅主题的分区数发生变化



**再平衡**

再均衡的核心目的是确保消费者能够在消费组内动态调整自己的分区分配，保证负载均衡和高可用。再均衡的四个阶段包括：

1. 查找 GroupCoordinator；
2. 加入消费组并选举 leader 和分配策略；在这个阶段，消费者和 GroupCoordinator 会进行以下操作：
   - **选举消费组 leader**：消费组内的第一个消费者默认成为 leader，如果有消费者退出，重新选举 leader。
   - **选举分区分配策略**：消费者会投票选出一个分区分配策略，通常是基于消费者支持的策略来选举出最适合的方案。
3. 根据分配策略把每个消费者对应的分区 通过 GroupCoordinator 传递给所有消费者。
4. 消费者通过心跳维持活跃状态，防止超时被认为宕机。



## Kraft

由于重度依赖Zookeeper集群，当Zookeeper集群性能发生抖动时，Kafka的性能也会收到很大的影响。因此，在Kafka发展的过程当中，为了解决这个问题，提供KRaft模式，来取消Kafka对Zookeeper的依赖。