---
layout: post
category: Mq
title: RocketMQ原理
tags: Mq
---

## RocketMQ原理

> [参考](https://www.modb.pro/db/72492)

Broker Cluster就是各个RocketMQ进程，Producer Cluster和Consumer Cluster分别是生产者和消费者，NameServer Cluster是路由中心。

<img src="https://cdn.jsdelivr.net/gh/mafulong/mdPic@vv3/v3/20211228135536.png" alt="img" style="zoom:50%;" />

Broker负责消息的存储，主从架构，定时和NameServer通信心跳保活。

Broker主从的master是通过raft算法选举出来的。



**NameServer没有master，每个都是**。NameServer是**无状态**的，即NameServer集群中的各个节点间是无差异的，**各节点间相互不进行信息通讯**。。NameServer是RocketMQ的路由中心，每一个NameServer节点都保存着**全量**的路由信息。因为Broker是集群部署，所以当生产者发送消息时，需要知道将消息发送到哪个Broker，当消息者获取消息时，也需要知道从哪个Broker获取消息。

**NameServer负责消息该路由到哪里，和Broker定时心跳。**每一个Broker节点（包括Slave）都会通过**心跳机制**（TCP长连接），将自己的基本信息注册到每一个NameServer中，这样Producer和Consumer就可以从NameServer拉取到路由消息。NameServer中有⼀个定时任务，每隔10秒就会扫描⼀次Broker表，查看每一个Broker的最新心跳时间戳距离当前时间是否超过120秒，如果超过，则会判定Broker失效，然后将其从Broker列表中剔除。

RocketMQ的路由发现采用的是Pull模型。当Topic路由信息出现变化时，NameServer不会主动推送给客户端，而是客户端定时拉取主题最新的路由。





RocketMq也是一个topic分了很多queue,其中一个queue是有序的。 queue类似kafka的partition。

除了Topic外，还有一个Tag分类，区分在于 Topic 是一级分类，而 Tag 可以理解为是二级分类。





RocketMQ之所以具有可扩展性，是因为每个Broker节点只保存整体数据的一部分，这样当数据量越来越大时，可以进行水平切分。

Broker在存储消息时，每一个Topic中的所有消息数据可能会分散在不同的Broker节点上，我们可以在创建Topic时进行指定。比如，假设我们的topic_orderInfo包含900万条消息，我们指定其分散在3个Broker节点上，那么每个节点就包含300万条消息数据。



Broker节点集群是一个**主备集群**，即集群中具有Master与Slave两种角色。Master负责处理读写操作请求，Slave负责对Master中的数据进行备份。当Master挂掉了，Slave则会自动切换为Master去工作。Master和Slave是1：N的关系（N >= 1）

另外要注意：**生产者只能往Master-Broker节点发送消息，消费既可以从Master-Broker节点消费消息，也可以从Slave-Broker节点消费消息**，



## QA

### **不使用zookeeper的原因：**

1. NameServer是自己写的，方便扩展，去中心化，只要有一个NameServer在，整个注册中心环境就可以用。
2. Zookeeper是CP的，在进行选举的时候，整个选举的时间太长，期间整个集群都处于不可用的状态，而这对于一个注册中心来说肯定是不能接受的。
3. 用Zookeeper架构更复杂，部署需要占用单独的服务器。



### Consumer获取消息的方式？

**拉取式消费**
Consumer主动从Broker中拉取消息，主动权由Consumer控制。一旦获取了批量消息，就会启动消费过程。不过，该方式的实时性较弱，即Broker中有了新的消息时消费者并不能及时发现并消费。

> 由于拉取时间间隔是由用户指定的，所以在设置该间隔时需要注意平稳：间隔太短，空请求比例会增加；间隔太长，消息的实时性太差

**推送式消费（项目就采用这种模式）**
该模式下Broker收到数据后会主动推送给Consumer。该获取方式一般实时性较高。
该获取方式是典型的**发布-订阅**模式，即Consumer向其关联的Queue注册了监听器，一旦发现有新的消息到来就会触发回调的执行，回调方法是Consumer去Queue中拉取消息。而这些都是基于Consumer与Broker间的长连接的。长连接的维护是需要消耗系统资源的。

> **pull**：需要应用去实现对关联Queue的遍历，实时性差；但便于应用控制消息的拉取 **push**：封装了对关联Queue的遍历，实时性强，但会占用较多的系统资源



### Consumer消费消息的模式？



广播模式: 

- 广播消费模式下，相同Consumer Group的每个Consumer实例都接收同一个Topic的全量消息。即**每条消息**都会被发送到Consumer Group中的**每个**Consumer。
- **消费进度**保存在**consumer**端。因为广播模式下consumer group中每个consumer都会消费所有消息，但它们的消费进度是不同。所以consumer各自保存各自的消费进度。



**集群消费**

1. 集群消费模式下，相同Consumer Group的每个Consumer实例平均分摊同一个Topic的消息。即**每条消息**只会被发送到Consumer Group中的**某个**Consumer。
2. **消费进度**保存在**broker**中。consumer group中的所有consumer共同消费同一个Topic中的消息，同一条消息只会被消费一次。消费进度会参与到了消费的负载均衡中，故消费进度是需要共享的。





## 和Kafka异同

> [参考](https://juejin.cn/post/6844903920058236936)

### 相同之处

- 两者均利用了操作系统Page Cache的机制，同时尽可能通过顺序io降低读写的随机性，将读写集中在很小的范围内，减少缺页中断，进而减少了对磁盘的访问，提高了性能。
- Kafka的Partition = RocketMq的Queue. 单个Queue是有序的，是分片。RocketMQ官方虽宣称支持严格有序，但方式为使用单个分区。
- RocketMq也支持延迟消息。
- 都支持事务。 RocketMQ支持事务消息，采用二阶段提交+broker定时回查。
- 都用了**顺序存储、Page Cache、异步刷盘、零拷贝**
- RocketMQ也有Consumer Group概念

### 不同之处

#### 存储形式

- Kafka采用partition，每个topic的每个partition对应一个文件。顺序写入，定时刷盘。但一旦单个broker的partition过多，则顺序写将退化为随机写，Page Cache脏页过多，频繁触发缺页中断，性能大幅下降。
- RocketMQ采用CommitLog+ConsumeQueue，单个broker所有topic在CommitLog中顺序写，Page Cache只需保持最新的页面即可。同时每个topic下的每个queue都有一个对应的ConsumeQueue文件作为索引。ConsumeQueue占用Page Cache极少，刷盘影响较小。

#### 存储可靠性

- RocketMQ支持异步刷盘，同步刷盘，同步Replication，异步Replication。
- Kafka使用异步刷盘，异步Replication。这里的异步刷盘指pageCache更新到磁盘里，比如命令fsync

#### 消息重复

- RocketMQ仅支持At Least Once。
- Kafka支持At Least Once、Exactly Once。

#### 消息过滤

- RocketMQ执行过滤是在Broker端，支持tag过滤及自定义过滤逻辑。
- Kafka不支持Broker端的消息过滤，需要在消费端自定义实现。

#### 消费

- RocketMQ支持按照时间回溯消费，实现原理与Kafka相同。
- Kafka需要先根据时间戳找到offset，然后从offset开始消费。
- RocketMQ相比Kafka多了个广播消费。广播消费模式下，相同Consumer Group的每个Consumer实例都接收同一个Topic的全量消息。即**每条消息**都会被发送到Consumer Group中的**每个**Consumer。**消费进度**保存在**consumer**端。因为广播模式下consumer group中每个consumer都会消费所有消息，但它们的消费进度是不同。所以consumer各自保存各自的消费进度。

#### 服务发现

- RocketMQ自己实现了namesrv。
- Kafka使用ZooKeeper。

#### 高可用

- RocketMQ在高可用设计上粒度只控制在Broker。其保证高可用是通过master-slave主从复制来解决的。
- Kafka控制高可用的粒度是放在分区上。每个topic的leader分区和replica分区都可以在所有broker上负载均衡的存储。
- Kafka的这种设计相比RocketMQ这种主从复制的设计有以下好处：
  - Kafka中不需要设置从broker，所有的broker都可以收发消息。负载均衡也做的更好。
  - Kafka的分区选举是自动做的，RocketMQ需要自己指定主从关系。
  - Kafka分区的复制份数指定为N，则可以容忍N-1个节点的故障。发生故障只需要分区leader选举下即可，效率很高。


