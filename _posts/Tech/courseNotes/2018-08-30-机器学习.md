---
layout: post
category: CourseNotes
title: 机器学习笔记
---

>专业选修，未曾深入，以下仅作回忆之用，不曾含有什么大的知识点

## 机器学习简介及线性回归模型

**1.机器学习的定义**

- 假设用P来评估计算机程序在某任务类T上的性能，若一个程序通过利用经验E在T中任务上获得了性能改善，则我们就说关于T和P，，改程序对E进行了学习。 —— [ Mitchell,1997 ]

**2.机器学习算法分类**

1. 监督学习：对于训练的数据集，标示明确的实际结果（如标明样本的房价，肿瘤的良性与恶性）。可分为：回归(连续值)和分类(离散值)
2. 非监督学习：在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。非监督学习中，数据将会被分为不同的cluster(簇),称为cluster algorithm。如新闻网页的专题分类。 

**3.单变量线性回归**

算法的工作原理如下图。用训练集“喂养”我们的学习算法，形成假设函数h。然后，对输入的x值，输出相应的预测值y。相当于是存在一个映射关系：y=f(x) 

![](https://img-blog.csdn.net/20170905204847129?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWVVORkVJWUFIRw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

代价函数：用来选择最合适的曲线。在假设函数hθ中,有两个未知量，选择不同的参数值，最终的效果肯定是不一样的

这样我们就得到了代价函数(cost function)。此处也称为平方误差函数，当然也有其他的代价函数。但是对于大多数问题，尤其是回归问题，平方误差函数都是一个合理的选择，“其可能是解决回归问题最常用的手段了”。 
如此，我们的目标就是要让代价函数最小

![](http://7xrrje.com1.z0.glb.clouddn.com/screenshot_11.png?imageMogr/v2/thumbnail/!45p)

**梯度下降算法**：

梯度下降算法是一种优化算法, 它可以帮助我们找到一个函数的局部极小值点。 

首先要知道方向导数、偏导数与梯度的概念。它们均涉及到函数的变化率，也就是增长的问题。对于高维函数，偏导数只是函数在坐标轴方向的变化率，但是很明显，函数可以有无数个方向（在xy平面内考虑）的变化，也就是方向导数。梯度，则是这所有中函数值增长最快的方向，考虑山丘地形，意味着最陡峭的地方。那么，我们的代价函数目标则是求最小值，只要沿着梯度相反方向，就可以最快到达目的地了。 

算法描述： 

![](http://7xrrje.com1.z0.glb.clouddn.com/screenshot_21.png?imageMogr/v2/thumbnail/!45p)

## 多元线性回归及正规公式

|梯度下降	|正规方程式|
| ------ | ------ | ------ |
|需要选择步长α	|不需要选择步长α|
|需要迭代训练很多次	|一次都不需要迭代训练|
|O(kn2)	|O(n3,计算(XT·X)-1需要花费较长时间|

即使数据特征n很大，也可以正常工作	n如果过大，计算会消耗大量时间

正规公式就是利用成本函数对参数求导为0时计算出来的

## 逻辑回归&正则化参数
这里需要使用到sigmoid函数–g(z)：

![](http://opn1dyhml.bkt.clouddn.com/17-8-2/66871114.jpg)

那个逻辑函数，也就是s型，激活函数，映射出0/1

因此可以利用logistic回归求出概率或者0/1分类，应用到多分类时，就需要对应数量的logistic回归

**逻辑回归的损失函数**

这里之所以再次提到损失函数，是因为线性回归中的损失函数会使得输出呈现起伏，造成许多局部最优值，也就是说线性回归中的cost function在运用到逻辑回归时，将可能不再是凸函数。

![](http://opn1dyhml.bkt.clouddn.com/17-8-2/52310223.jpg)

由上图可知，y=1，hθ(x)是预测值， 

- 当其值为1时，表示预测正确，损失函数为0； 
- 当其值为0时，表示错的一塌糊涂，需要大大的惩罚，所以损失函数趋近于∞。

### 过拟合
主要说一下过拟合的解决办法： 

1）减少特征数量

手动选择一些需要保留的特征

使用模型选择算法（model selection algorithm）

2）正则化

保留所有特征，但是参数θ的数量级（大小）要减小

当我们有很多特征，而且这些特征对于预测多多少少会由影响，此时正则化怎能起到很大的作用。

![](http://opn1dyhml.bkt.clouddn.com/17-8-3/31790906.jpg)

## 神经网络

![](http://opn1dyhml.bkt.clouddn.com/17-8-3/54262078.jpg)

如图是一个三层结构的神经网络（输入层，隐藏层、输出层），每一层的激活单元的计算表达式图中也已经写出来了。 

Cost Function： 在神经网络中损失函数略微复杂了些,但是也比较好理解，就是把所有层都算进去了。 

神经网络不能0初始化，用来分类，输出0/1，输出层的数量为1时，就一个输出变量0/1，多个就多分了，矩阵里只有1个单元为1。

**BP**

首先需要知道的是BP算法是干嘛的？它是用来让神经网络自动更新权重W的。 
这里权重W与之前线性回归权值更新形式上是一样：

后向传播算法是深度学习中一种训练与学习方法，用来调整深度网络中各个节点之间的权重，使得网络输出的样本标签与实际标签相一致。

![](http://opn1dyhml.bkt.clouddn.com/17-8-5/53630966.jpg)

![](https://img-blog.csdn.net/20170808174628799?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbWFyc2dnYm8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

## SVM

SVM就是最大间隔，求出一条线划分区域为2部分，或者多部分。最大化分割线与点的距离

## K-means
聚类，分成k个部分，不停迭代计算每个点与聚点的距离然后更新位置

## PCA主成分分析
主要用于数据降维，高维变成低维，最小化特征损失

## 准确率和召回率

对于癌症检测，召回率越高越好，为了防止漏查