---
layout: post
category: DistributedSystem
title: etcd和raft
tags: DistributedSystem
recent_update: true
---

# etcd

# raft

不同于Paxos算法直接从分布式一致性问题出发推导出来，Raft算法则是从多副本状态机的角度提出，用于管理多副本状态机的日志复制。Raft实现了和Paxos相同的功能，它将一致性分解为多个子问题：Leader选举（Leader election）、日志同步（Log replication）、安全性（Safety）、日志压缩（Log compaction）、成员变更（Membership change）等。同时，Raft算法使用了更强的假设来减少了需要考虑的状态，使之变的易于理解和实现。



这个raft文档很清晰， [参考](https://juejin.cn/post/6907151199141625870#heading-22)

## 状态

Raft将系统中的角色分为领导者（Leader）、跟从者（Follower）和候选人（Candidate）：

- Leader：接受客户端请求，并向Follower同步请求日志，当日志同步到大多数节点上后告诉Follower提交日志。
- Follower：接受并持久化Leader同步的日志，在Leader告之日志可以提交之后，提交日志。
- Candidate：Leader选举过程中的临时角色。

Follower只响应其他服务器的请求。如果Follower超时没有收到Leader的消息，它会成为一个Candidate并且开始一次Leader选举。收到大多数服务器投票的Candidate会成为新的Leader。Leader在宕机之前会一直保持Leader的状态



状态机

<img src="https://cdn.jsdelivr.net/gh/mafulong/mdPic@vv3/v3/20211226171751.png" alt="image-20211226171744055" style="zoom:50%;" />

term：任期，比如新的选举任期，即整个集群初始化时，或者新的Leader选举就会开始一个新的选举任期。

Raft算法将时间分为一个个的任期（term），**每一个term的开始都是Leader选举**。在成功选举Leader之后，Leader会在整个term内管理整个集群。如果Leader选举失败，该term就会因为没有Leader而结束。



## Leader选举

   Raft 使用心跳（heartbeat）触发Leader选举。当服务器启动时，初始化为Follower。Leader向所有Followers周期性发送heartbeat。如果Follower在选举超时时间内没有收到Leader的heartbeat，就会等待一段随机的时间后发起一次Leader选举。

   Follower将其当前term加一然后转换为Candidate。它首先给自己投票并且给集群中的其他服务器发送 RequestVote RPC。结果有以下三种情况：

- 赢得了多数的选票，成功选举为Leader；
- 收到了Leader的消息，表示有其它服务器已经抢先当选了Leader；
- 没有服务器赢得多数的选票，Leader选举失败，等待选举时间超时后发起下一次选举。

如果 Candidate 在一定时间内没有获得足够的投票，那么就会进行一轮新的选举，直到其成为 Leader,或者其他结点成为了新的 Leader，自己变成 Follower。

<img src="https://cdn.jsdelivr.net/gh/mafulong/mdPic@vv3/v3/20211226171954.png" alt="image-20211226171954920" style="zoom:50%;" />

选举出Leader后，Leader通过定期向所有Followers发送心跳信息维持其统治。若Follower一段时间未收到Leader的心跳则认为Leader可能已经挂了，再次发起Leader选举过程。

Raft保证选举出的Leader上一定具有最新的已提交的日志。

## 日志同步

Leader选出后，就开始接收客户端的请求。Leader把请求作为日志条目（Log entries）加入到它的日志中，然后并行的向其他服务器发起 AppendEntries RPC 复制日志条目。当这条日志被复制到超过一半的大多数服务器上，Leader将这条日志应用到它的状态机并向客户端返回执行结果。然后再向followers请求提交数据。

<img src="https://cdn.jsdelivr.net/gh/mafulong/mdPic@vv3/v3/20211226172121.png" alt="image-20211226172121245" style="zoom:50%;" />

某些Followers可能没有成功的复制日志，Leader会无限的重试 AppendEntries RPC直到所有的Followers最终存储了所有的日志条目。

日志由有序编号（log index）的日志条目组成。每个日志条目包含它被创建时的任期号（term），和用于状态机执行的命令。如果一个日志条目被复制到大多数服务器上，就被认为可以提交（commit）了。

- log：Raft系统中的主要工作单元是log     entry。一致性问题可以分解为*replicated log*。log是entry的有序序列，entry包括任何集群更改：添加节点，添加服务，新的键值对等。如果所有成员都同意entry及其顺序，则我们认为log是一致的。
- Quorum：法定人数是来自集群的大多数成员：对于一组大小n，法定人数至少需要(n+1)/2成员。例如，如果在集群中有5个成员，我们至少需要3个节点来形成仲裁。如果由于某些原因无法达到法定数量的节点，则群集将变得*不可用，*并且无法提交新日志。
- Committed Entry:     将条目持久存储在一定数量的节点后，该条目被视为已*提交*。条目被提交后即可应用。

leader扮演的是分布式事务中的协调者，接受客户端的写请求，每次有数据更新的时候产生二段提交。在leader收到数据操作的请求，先不着急更新本地数据，而是生成对应的log，然后把生成log的请求广播给所有的follower。当leader收到超过半数的follower成功写了log的影响，则开始第二阶段的提交：正式写入数据，然后同样广播给follower。

WAL技术+2PC

### 如何提交日志

可以把 Raft 的日志复制理解成一个优化后的二阶段提交

首先，领导者进入第一阶段，通过日志复制（AppendEntries）RPC 消息，将日志项复制到集群其他节点上。接着，如果领导者接收到大多数的“复制成功”响应后，它将日志项应用到它的状态机，并返回成功给客户端。

如果领导者没有接收到大多数的“复制成功”响应，那么就返回错误给客户端。学到这里，有同学可能有这样的疑问了，领导者将日志项应用到它的状态机，怎么没通知跟随者应用日志项呢？

这是 Raft 中的一个优化，领导者不直接发送消息通知其他节点应用指定日志项。因为领导者的日志复制 RPC 消息或心跳消息，包含了当前最大的，将会被提交（Commit）的日志项索引值。

所以通过日志复制 RPC 消息或心跳消息，跟随者就可以知道领导者的日志提交位置信息。因此，当其他节点接受领导者的心跳消息，或者新的日志复制 RPC 消息后，就会将这条日志项应用到它的状态机。

而这个优化，降低了处理客户端请求的延迟，将二阶段提交优化为了一段提交，降低了一半的消息延迟。为了帮你理解，我画了一张过程图，然后再带你走一遍这个过程，这样你可以更加全面地掌握日志复制。

 

通过心跳告诉日志已经提交。

<img src="https://cdn.jsdelivr.net/gh/mafulong/mdPic@vv5/v5/20220113002527.png" alt="image-20211226172620615" style="zoom:50%;" />

1. 接收到客户端请求后，领导者基于客户端请求中的指令，创建一个新日志项，并附加到本地日志中。
2. 领导者通过日志复制     RPC，将新的日志项复制到其他的服务器。
3. 当领导者将日志项，成功复制到大多数的服务器上的时候，领导者会将这条日志项应用到它的状态机中。
4. 领导者将执行的结果返回给客户端。
5. 当跟随者接收到心跳信息，或者新的日志复制     RPC 消息后，如果跟随者发现领导者已经提交了某条日志项，而它还没应用，那么跟随者就将这条日志项应用到本地的状态机中。





## 安全性保障（核心）

Raft算法中引入了如下两条规则，来确保了

- **已经commit的消息，一定会存在于后续的Leader节点上，并且绝对不会在后续操作中被删除。**
- 对于并未commit的消息，可能会丢失。



**投票规则**

**每个 candidate 必须在 RequestVote RPC 中携带自己本地日志的最新 (term, index)，如果 follower 发现这个 candidate 的日志还没有自己的新，则拒绝投票给该 candidate。**



Candidate 想要赢得选举成为 leader，必须得到集群大多数节点的投票，那么**它的日志就一定至少不落后于大多数节点**。又因为一条日志只有复制到了大多数节点才能被 commit，因此**能赢得选举的 candidate 一定拥有所有 committed 日志**。

因此我们才会断定地说：Follower 不可能比 leader 多出一些 committed 日志。

比较两个 (term, index) 的逻辑非常简单：如果 term 不同 term 更大的日志更新，否则 index 大的日志更新。





**提交规则**



什么是 commit：

> 当 leader 得知某条日志被集群过半的节点复制成功时，就可以进行 commit，committed 日志一定最终会被状态机 apply。



•对于一个给定的任期号，最多只会有一个Leader被选举出来（章节5.2)

•Leader绝对不会删除或者覆盖自己的日志，只会增加（章节5.3）

•如果两个日志在某一相同索引位置日志条目的任期号相同，那么我们就认为这两个日志从头到该索引位置之间的内容完全一致（章节5.3）

•如果某个日志条目在某个任期号中已经被提交，那么这个条目必然出现在更大任期号的所有领导人中（章节5.4）

•如果某一服务器已将给定索引位置的日志条目应用至其状态机中，则其他任何服务器在该索引位置不会应用不同的日志条目（章节5.4.3）





上面说到，只要日志在多数结点上存在，那么 Leader 就可以提交该操作。但是**Raft额外限制了 Leader只对自己任期内的日志条目适用该规则，先前任期的条目只能由当前任期的提交而间接被提交。** 也就是说，当前任期的Leader，不会去负责之前term的日志提交，之前term的日志提交，只会随着当前term的日志提交而间接提交。

- 一个候选者在发起选举时，必须包含自己的日志中的最后一个条目的索引和任期。一个节点在投票给候选者时，必须检查候选者的日志是否比自己的日志更新。这样，可以保证选出的领导者的日志是最新的。
- 一个领导者在复制日志时，必须包含自己的日志中的最后一个条目的索引和任期。一个节点在接受日志时，必须检查领导者的日志是否与自己的日志匹配。这样，可以保证领导者的日志是一致的。
- 一个领导者在提交日志时，必须保证自己的任期与日志条目的任期相同。这样，可以保证领导者不会提交过期的日志条目。
- **Leader 只允许 commit 包含当前 term 的日志。** 否则出现[问题c](https://juejin.cn/post/6901246901287387150),   





看日志提交过程， 会不会刚本地提交，其它follower还没提交，但有日志，如何处理？

- 新Leader直接根据日志在多数节点存在的这个规则，会将之前term的日志提交。 但这个只是间接提交，Leader只允许commit包含当前term的日志。这样提交的副本都是最新term，不会被新覆盖而丢失。




**安全性总结**

- 日志顺序写，保持最新，如果有不一致，会被leader的日志覆盖。
- 选举时只有最新日志的才能参与选举，防止旧的follower当选leader而导致丢信息。
- 已经commit的消息，一定会存在于后续的Leader节点上，并且绝对不会在后续操作中被删除。对于并未commit的消息，可能会丢失。日志存在大多数节点上就认为是commit，但Leader提交时必须包含当前term。 
- 防止脑裂，Raft leader拥有最新的数据，因此读请求走leader那么leader可以直接返回结果给客户端，但直接从leader状态机读并不能完全确保系统的线性一致性。比如发生网络分区，旧leader处于少数派分区中，此时直接从旧leader的状态机读，则很可能返回过期的数据。所以leader需要在发起读时记录当前的committed Index，然后在后续heartbeat请求中如果能获得多数follower对leadership的确认，那么可以等待committed Index提交到状态机后即可返回结果。记录committed Index是因为当一个节点刚当选 Leader的时候并不知道最新的committed index。 









## 强一致性读的保证：Raft 的线性一致读方法

Raft 的 Leader 通常拥有最新的数据，因此读请求可以直接通过 Leader 来处理。然而，仅从 Leader 的状态机读取数据并不能完全确保线性一致性。以下情况可能破坏一致性：

- **网络分区**：在某些情况下，旧 Leader 可能位于少数派分区中，仍然接收读请求。这时，直接从旧 Leader 读取数据可能会返回过期的值。

### 1. Read Committed Index

Raft 的 Leader 通常拥有最新的数据，因此读请求可以直接通过 Leader 来处理。然而，仅从 Leader 的状态机读取数据并不能完全确保线性一致性。以下情况可能破坏一致性：

- **网络分区**：在某些情况下，旧 Leader 可能位于少数派分区中，仍然接收读请求。这时，直接从旧 Leader 读取数据可能会返回过期的值。

为了解决这个问题，Leader 在处理线性一致读时，会执行以下步骤：

1. **记录当前的 committed Index**：Leader 在收到读请求时，记录当前的 committed Index（即最新被多数节点确认的日志索引）。
2. **确认领导权**：Leader 在后续的心跳（heartbeat）请求中，向多数派 Follower 确认自己仍然是合法的 Leader。
3. **等待 committed Index 提交到状态机**：在确认领导权后，Leader 等待记录的 committed Index 对应的日志条目已应用到状态机，然后返回结果。

这种方式确保了即使在网络分区或领导权切换等场景下，读请求也能维持线性一致性。  这个和写成本差不多。



### 2. Follower Read

ETCD 中实现了一种更加高效的线性一致性读方法，即 **Follower Read**。它允许读请求通过 Follower 来处理，同时保证强一致性。具体步骤如下：

1. 向 Leader 查询 committed Index：
   - Follower 收到客户端读请求后，先向当前的 Leader 查询最新的 committed Index。
2. 等待自身 committed Index 同步
   - Follower 等待自身的 committed Index 增长到 Leader 返回的值（即读请求发生时 Leader 的 committed Index）。
3. 返回最新数据
   - 当 Follower 的状态机应用到对应的日志后，直接从本地读取数据并返回结果。

我认为当前etcd就实现的Follower read可能是更有效的一种一致性读的实现方式。读请求可以发给follower，follower先去leader查询最新的committed index，然后等待自身的committed index增长为读请求发生时的leader的committed index，从而保证能从follower中读到最新的数据。



注意：Follower Read 并不意味着我们在读过程中完全不依赖 leader 了，在保证线性一致性的前提下完全不依赖 leader 理论上是不可能做到的。



## 集群成员变更

在前文的理论描述中我们都假设了集群成员是不变的，然而在实践中有时会需要替换宕机机器或者改变复制级别（即增减节点）。一种最简单暴力达成目的的方式就是：停止集群、改变成员、启动集群。这种方式在执行时会导致集群整体不可用，此外还存在手工操作带来的风险。

为了避免这样的问题，Raft 论文中给出了一种无需停机的、自动化的改变集群成员的方式，其实本质上还是利用了 Raft 的核心算法，**将集群成员配置作为一个特殊日志从 leader 节点同步到其它节点去。**



先说结论：**所有将集群从旧配置直接完全切换到新配置的方案都是不安全的**。



**两阶段切换集群成员配置**

Raft 使用一种两阶段方法平滑切换集群成员配置来避免遇到前一节描述的问题。具体流程如下：

**阶段一**

1. 客户端将 C-new 发送给 leader，leader 将 C-old 与 C-new 取**并集**并立即apply，我们表示为 **C-old,new**。
2. Leader 将 C-old,new 包装为日志同步给其它节点。
3. Follower 收到 C-old,new 后立即 apply，当 **C-old,new 的大多数节点（即 C-old 的大多数节点和 C-new 的大多数节点）**都切换后，leader 将该日志 commit。

**阶段二**

1. Leader 接着将 C-new 包装为日志同步给其它节点。
2. Follower 收到 C-new 后立即 apply，如果此时发现自己不在 C-new 列表，则主动退出集群。
3. Leader 确认 **C-new 的大多数节点**都切换成功后，给客户端发送执行成功的响应。



## 日志压缩

我们知道 Raft 核心算法维护了日志的一致性，通过 apply 日志我们也就得到了一致的状态机，客户端的操作命令会被包装成日志交给 Raft 处理。然而在实际系统中，客户端操作是连绵不断的，但日志却不能无限增长，首先它会占用很高的存储空间，其次每次系统重启时都需要完整回放一遍所有日志才能得到最新的状态机。

因此 Raft 提供了一种机制去清除日志里积累的陈旧信息，叫做**日志压缩**。

**快照**（**Snapshot**）是一种常用的、简单的日志压缩方式，ZooKeeper、Chubby 等系统都在用。简单来说，就是将某一时刻系统的状态 dump 下来并落地存储，这样该时刻之前的所有日志就都可以丢弃了。所以大家对“压缩”一词不要产生错误理解，我们并没有办法将状态机快照“解压缩”回日志序列。

注意，**在 Raft 中我们只能为 committed 日志做 snapshot**，因为只有 committed 日志才是确保最终会应用到状态机的。



快照一般包含以下内容：

1. **日志的元数据**：最后一条被该快照 apply 的日志 term 及 index
2. **状态机**：前边全部日志 apply 后最终得到的状态机

当 leader 需要给某个 follower 同步一些旧日志，但这些日志已经被 leader 做了快照并删除掉了时，leader 就需要把该快照发送给 follower。

同样，当集群中有新节点加入，或者某个节点宕机太久落后了太多日志时，leader 也可以直接发送快照，大量节约日志传输和回放时间。



# Raft vs Paxos

共同点: 

- **服务器都会存储一个递增的数字叫做 Term（任期号），每条 RPC 消息都会包含 Term。当服务器接收到 RPC 时，它会检查 RPC 里包含的 Term 与自身 Term 的大小**
- 都有3种类型。 服务器最初都是 Candidate 状态，它们会尝试互相发送 RequestVote RPC  当选 Leader。有一台 Candidate 会成为 Leader，其他服务器会成为 Follower。当 Follower 认为 Leader 失败，又会成为 Candidate 进行选举。Leader 节点必须定期发送 AppendEntries RPC 作为 keepalive，以防止 Follower 超时成为 Candidate。



差异点: 

- **Paxos 允许任何服务器成为 Leader，然后再从其他服务器补齐缺少的日志，而 Raft 只允许拥有最新日志的服务器成为 Leader。**

- **在 Raft 中，日志条目的 term 不会被未来的 leader 所修改。而在 Paxos 协议下，这是可能的。**

  在 Raft 中，新的 Leader 选举出来后，对待上一个 term 未提交的日志条目，会继续沿用原有的 term，复制到其他节点。**事实上，Raft Leader 甚至永远不会覆盖或删除其日志中的条目，只会追加写新条目。**

- **Raft 的日志是严格顺序写入的，而 Paxos 通常允许无序写入，但需要额外的协议来填充可能因此发生的日志间隙。** 无序写入的优势是可以实现更高的并发性和性能。但是,这需要付出额外的复杂性代价来处理日志的不一致情况。相反，Raft 的严格顺序写入降低了这种复杂性，代价是牺牲一定的并发性。

- 在 Raft 协议里，即使日志条目被多数派的节点所接受（但未提交），也有可能被回滚掉。只有新选举出来的 Leader 将当前 term 的第一个日志条目写到多数派服务器上，上一个 term 的日志条目才可以安全提交。而在 Paxos 系统里，只要日志条目被复制到了多数派服务器上，Paxos 就可以安全地提交日志条目。



raft优势：简单，好懂，易实现。

RAFT的简单有好几个方面，比如：

1）协议上的简化（最后主要RPC只有两个，其他协议的二阶段、三阶段也都变成 /看起来像是一阶段）； 

2）TERM概念的强化（看起来似乎PAXOS也有重选LEADER的机制，但是强化概念，并增加一个TERM内有一个一个LEADER、ENTRY与TERM相关的属性等都大大简化了流程）；

3）LOG只会从LEADER到FOLLOWER单向同步（实现一下你会发现这真的减少好多问题，代码量都少了好多）；

