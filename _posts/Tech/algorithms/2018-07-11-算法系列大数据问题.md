---
layout: post
category: 算法知识
title: 算法系列大数据问题
---
注意哈希的使用，哈希不等，一般就不相等了

1. 分治 + 哈希
2. 位图扩展
3. 分治 + 比对



## 在超大文件中找出访问百度次数最多的IP

### 题目描述
现有海量日志数据保存在一个超级大的文件中，该文件无法直接读入内存，要求从中提取某天出访问百度次数最多的那个IP

### 思考过程
（1）面试中若题目提到大文件等，其实就是告诉你数据量大，不能一次性加载到内存中，而实际中我们就需要估算。既然是要对访问百度次数的ip做统计，我们最好先预处理一下，遍历把访问百度的所有ip写到另一个文件a中

（2）ip用32位表示，所以最多有2^32个不同ip地址。同样的，当内存不能一次性加载数据时，我们就需要考虑分治法。

step1：采用hash映射(hash(ip)%1000)分别把结果保存到小文件a0....a999中。有人可能会问,这里一定要用1000吗？当然不一定，需要估算，比如若文件a总共320G远远大于4G内存，我们就需要分块（hash映射），若分为1000块，则每块大约300M，再读入内存就没问题了。

step2：可以采用hash_map进行频率统计，找出每个小文件中出现频率最大的IP。对于每一个小文件ai，具体操作如下：创建hash_map,遍历小文件中每条记录。对于每条记录，先在hash_map中搜索，若有，将hash_map中记录count+1，若没有，插入hash_map

step3:在这1000个最大的IP中，找出count最大的ip

## 查找出现次数在前K的IP地址
### 题目描述
如题

### 思考过程

和上一个题目类似，不同的是

将所有文件的出现次数前K个IP地址放在一起，再找出前K个IP地址，就是所要求的IP，至此问题解决。

关于堆实现Top K：有一堆海量数据，需要求最大的K个数，排序太耗时间，这个时候用的即堆，建一个元素个数为K个的堆，由于要找最大的N个数，所以要建小堆（没错就是小堆），然后依次从文件读取数据，当读入数据大于堆顶元素，就取代堆顶元素，然后重新调整为小堆，直至元素读取完成。

## 给两个文件，分别有100亿个整数，我们只有1G内存，如何找到两个文件的交集？

### 题目描述

给两个文件，分别有100亿个整数，我们只有1G内存，如何找到两个文件的交集？

### 思考过程
#### 位图 + 比对
我们知道对于整形数据来说，不管是有符号的还是无符号的，总共有2^32=4G个数据(100亿个数据中肯定存在重复的数据)，我们可以采用位图的方式来解决，假如我们用一个位来代表一个整形数据，那仫4G个数共占512M内存(八个数一个字节，4G/8=512M)。我们的做法是将第一个文件里的数据映射到位图中，再拿第二个文件中的数据和第一个文件中的数据做对比，有相同的数据就是存在交集(重复的数据，交集中只会出现一次).

![](https://img-blog.csdn.net/20170117170038310?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzQzMjg4MzM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

#### 分治 + 比对
整数无非是32位，将其按位切割，对文件1进行切割，第一位（也就是最高位）为1的切割进一个文件，第一位（最高位）为0进入另一个文件；
重复第1步，对切割的文件进行再次切割，第1次是第1位，第2次对第2位进行切割，直至切割完毕；
文件2每个数进行查找，由于已经进行切割，查找类似于二分查找，效率很高，最坏也不过查找32次。

## 假定一个文件有100亿个整形数据，1G内存，如何找到出现次数不超过两次的数字？

### 题目描述
假定一个文件有100亿个整形数据，1G内存，如何找到出现次数不超过两次的数字？

### 思考过程
分析：要解决这个问题同样需要用到位图的思想，在问题二中已经了解到采用位图的一个位可以判断数据是否存在，那仫找到出现次数不超过两次的数字使用一个位是无法解决的，在这里可以考虑采用两个位的位图来解决.

根据上述分析我们可以借助两个位，来表示数字的存在状态和存在次数，比如：00表示不存在，01表示存在一次，10表示存在两次，11表示存在超过两次；类似问题二的计算过程：如果一个数字占一位，需要512M内存即可，但是如果一个数字占两位，则需要(2^32)/(2^2)=2^30=1G内存；将所有数据映射到位图中查找不是11的所对应的数字就解决上述问题了。

题目扩展：其他条件不变，假如只给定512M内存该如何找到出现次数不超过两次的数字？

分析：将数据分批处理，假若给定的是有符号数，则先解决正数，再解决负数，此时512M正好解决上述问题.

类似的找出现一次的，出现两次的

## 给两个文件，分别有100亿个query，我们只有1G内存，如何找到两文件交集？分别给出精确算法和近似算法!

### 题目描述
给两个文件，分别有100亿个query，我们只有1G内存，如何找到两文件交集？分别给出精确算法和近似算法!

其实就是查找相同的字符串，但字符串没法子桶装了，因为数量无限啊，不像int那样32位

### 思考过程
看到字符串首先应该反应过来的就是布隆过滤器，而问题四的近似算法就是采用布隆过滤器的方法，之所以说布隆过滤器是近似的算法，因为它存在一定 的误判(不存在是肯定的，存在是不肯定的)；而要想精确判断字符串文件的交集，我们可以采用分而治之的方法：将大文件切分为一个一个的小文件，将一个又一个的小文件拿到内存中做对比，找到对应的交集。

#### 哈希切分的精确解决办法：

既然叫做切分，顾名思义就是将大文件切分为小文件，那仫如何切分？切分的依据是什仫呢？如果我们在切分的时候可以将相似或者相同的文件切分到同一个文件中那仫是不是就加快了查找交集的速度呢？答案是肯定的。

知道了哈希切分的依据我们应该如何处理呢？我们可以根据字符串的某个哈希算法得到该字符串的key，然后将key模要分割的文件数(假设为1000个文件，文件编号为0~999)，我们将结果相同的字符串放到同一个文件中(两个文件中的字符串通过相同的哈希算法就会被分到下标相同的文件中)，此时我们只需要将下标相同的文件进行比对就可以了。。。

也算是桶分组后分而治之了

## 给上千个文件，每个文件大小为1K~100M。给n个词，设计算法对每个词找到所有包含它的文件，你只有100K内存

### 题目描述
给上千个⽂文件，每个文件大小为1K~100M。给n个词，设计算法对每个词找到所有包含它的文件，你只有100K内存


#### 倒排索引

将n个单词构成一颗二叉搜索树，如果是中文，则先经过分词再构建；

对每个文件的每个词进行搜索，如果该词存在，将其文件名挂在对应单词后面，直至文件读取结束；

每个节点后所挂的文件信息列表即需要获取的信息

看图了解倒排索引

![](https://i.imgur.com/Z4GqmKN.png)

## 有一个词典，包含N个英文单词，现在任意给一个字符串，设计算法找出包含这个字符串的所有英文单词
### 题目描述
有一个词典，包含N个英文单词，现在任意给一个字符串，设计算法找出包含这个字符串的所有英文单词

### 思考过程
#### 字典树

字典树是一种特殊的树，根节点为空，每个节点只有一个字母，如图： 

![](https://i.imgur.com/5CmMjRl.png)
