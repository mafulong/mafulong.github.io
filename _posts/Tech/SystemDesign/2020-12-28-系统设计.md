---
layout: post
category: SystemDesign
title: 系统设计
tags: SystemDesign
---

## 系统设计- 参考



几本书：

- [谷歌面试准备完全指南 Done](https://wizardforcel.gitbooks.io/gainlo-interview-guide/content/sd6.html)

- [donnemartin/system-design-primer多语言翻译 Done](https://github.com/donnemartin/system-design-primer)
  - [中文版本](https://wizardforcel.gitbooks.io/system-design-primer/content/)

- [ 必看Grokking system design, repo无法访问](https://github.com/lei-hsia/grokking-system-design)

设计数据密集型应用-Data-Intensive Application

[https://github.com/donnemartin/interactive-coding-challenges](https://github.com/donnemartin/interactive-coding-challenges)

[两周系统设计面试速成大纲](https://acecodeinterview.com/2_week_prep/)





https://www.raychase.net/6364 

应用部分：

- [常见分布式应用系统设计图解（一）：即时消息系统](https://www.raychase.net/6260)
- [常见分布式应用系统设计图解（二）：Feed 流系统](https://www.raychase.net/6269)
- [常见分布式应用系统设计图解（三）：Top K 系统](https://www.raychase.net/6275)
- [常见分布式应用系统设计图解（四）：输入建议系统](https://www.raychase.net/6299)
- [常见分布式应用系统设计图解（五）：Proximity 系统](https://www.raychase.net/6312)
- [常见分布式应用系统设计图解（六）：流媒体系统](https://www.raychase.net/6329)
- [常见分布式应用系统设计图解（七）：爬虫搜索系统](https://www.raychase.net/6327)
- [常见分布式应用系统设计图解（八）：文件同步分享系统](https://www.raychase.net/6345)
- [常见分布式应用系统设计图解（九）：协同编辑系统](https://www.raychase.net/6429)
- [常见分布式应用系统设计图解（十）：电商秒杀系统](https://www.raychase.net/6434)
- [常见分布式应用系统设计图解（十一）：数据监控系统](https://www.raychase.net/6439)
- [常见分布式应用系统设计图解（十二）：证券交易系统](https://www.raychase.net/6453)
- [常见分布式应用系统设计图解（十三）：短网址系统](https://www.raychase.net/6460)
- [常见分布式应用系统设计图解（十四）：日志系统](https://www.raychase.net/7087)
- [常见分布式应用系统设计图解（十五）：支付系统](https://www.raychase.net/7140)

基础设施部分：

- [常见分布式基础设施系统设计图解（一）：分布式文件系统](https://www.raychase.net/6368)
- [常见分布式基础设施系统设计图解（二）：分布式数据库](https://www.raychase.net/6367)
- [常见分布式基础设施系统设计图解（三）：分布式消息队列](https://www.raychase.net/6396)
- [常见分布式基础设施系统设计图解（四）：分布式工作流系统](https://www.raychase.net/6407)
- [常见分布式基础设施系统设计图解（五）：分布式流控系统](https://www.raychase.net/6416)
- [常见分布式基础设施系统设计图解（六）：分布式 MR 系统](https://www.raychase.net/6422)
- [常见分布式基础设施系统设计图解（七）：分布式实时流处理系统](https://www.raychase.net/6444)
- [常见分布式基础设施系统设计图解（八）：分布式键值存储系统](https://www.raychase.net/7121)

## 答题流程



高可用，主要以下几个方面: 

- 数据持久化
- 主从复制
- 自动故障恢复
- 集群化



问面试官问题，即便很清晰也要问，将问题重新定义。

1. Users: who will use, how the system will be used?
2. Scale: qps, tps等 
3. Performance: the delay of write-to-read
4. Cost



需求重新定义：这里不需要问面试官

对于功能模块: 定义api: name以及req, resp

对于非功能需求: 规模、性能、可用性。

scalability, performance, availability(灾备、无单点)

consistency, cost



一个大模块就组织出来了。

数据模型，来存储。

各有什么优缺点。



SQL是否可以满足，如何满足的，这块Nosql满足的话成本是不是更少。



处理服务 processing service



### 时间分配

以40分钟的面试时间来算（掐头去尾除去自我介绍问问题），我面试的大概流程如下。

- 【3分钟】理解需求 （询问系统的商业目的 + 询问系统的功能和技术需求 + 定义成功）
  - Functional Requirements
    - 用户是否需要登录
    - 仅仅上传图片，不需要上传视频
    - 图片保存有效期

  - Non-functional Requirements
    - 基本可用性
    - 交互流畅、低延迟
    - 是否支持某个动作反馈有一定延迟

- 【0-5分钟】资源估算（optional)（计算需要多少台机器，需要多少内存硬盘和CPU的能力）
  - Capacity 可能非核心内容加上时间有限，做个大致计算估计的方法。
    - Traffic
      - qps xx times per day, xx times per second
      - tps, 按读写比10:1
      - peak qps: qps * 5

    - Storage
      - add data xx per day, xx per year
      - cache xx..

- 【5分钟】High-level diagram
  
- 划分模块
  
- 【5分钟】数据结构与存储

- 【10分钟】核心子服务设计

- 【5分钟】接口设计

- 【5分钟】扩展性，容错性，延迟要求
  
  - 架构扩展性：

    - 关注： 存储数据量， 存储吞吐量； 解决方案：存储拆分、单体吞吐低可以batch，多级缓存。
    - 服务： 吞吐压力： 先尝试扩容，无状态服务。否则降级： 保障核心服务，关闭弱依赖。
    - 伸缩性： 无状态，可横向扩容
    - 高峰期流量过高:  mq削峰
  
  - 业务可扩展性
  
    - 搞多app矩阵的支持，增加namespace, appId
  
    - 类型增加，增加ItemType
  
      
  
  - 容错性
    - 数据库挂了怎么办
      - 主从灾备
  
    - 接口挂了怎么办
      - 失败重试，缓存队列
      - 防止雪崩，要熔断
  
    - 降级、熔断
    - 异步
      - 消息队列 削锋
  
- 【2-7分钟】专题 deep dive
  - 用户scope变大
    - 多机房同步
    - 多语言

  - 某个时间点流量特别大了怎么办
    - 降级、熔断
    - 异步

  - 缓存
    - 大V设计


## 系统设计时间估算

[参考](https://www.raychase.net/6280)

1天86400s

每个字符占用 2 个字节

bit 是位，1 byte == 8 bits

B=byte, 1M = 1024byte

读写比默认10:1

数据类型的空间占用： 在很多系统中，Boolean 占用 1 个字节，字符占用 2 个字节，Integer/Float 是 4 个字节，Long/Double 则是 8 个字节。

#### 时间数量级

Jeff Dean 十年前有一个[著名的分享](https://research.cs.cornell.edu/ladis2009/talks/dean-keynote-ladis2009.pdf)，介绍了他认为重要的系统的数值。我觉在讨论多数系统来说，有这样几个关于时间的数值（参考数量级）比较常见（注意时间单位的关系：1 秒 = 1000 毫秒 (millisecond) = 1,000,000 微秒 (microsecond) = 1,000,000,000(nanosecond)）：

- CPU 访问（包括 CPU 缓存）：10 纳秒
- 内存访问：100 纳秒
- HDD 磁盘访问：10 毫秒，如果是 SSD 大约快 100 倍
- HDD 磁盘吞吐量：100 MB/s，如果是 SSD 则高几倍
- 同机房网络时延：1 毫秒
- 异地网络时延：10 毫秒
- 国际网络时延：100 毫秒

#### 单机吞吐量上限

- Web 服务器的 QPS：1000
- RDB 单机 QPS：1000
- NoSQL DB 磁盘单机 QPS：10K
- 内存访问单机 QPS：1M

## 系统设计常用算法

[参考](https://github.com/resumejob/system-design-algorithms)

- Bloom filter 
- count-min sketch bloom filter基础上增加计数，以最小那个hash计数为值
- Frugal Streaming 
  - Frugal Streaming uses only one unit of memory per group to compute a quantile for each group
- Geohash / S2 Geometry  地理位置
- Leaky bucket / Token bucket  限流器
- Lossy Counting ，top k频繁 hashmap不停轮换清counter为0的
- [Operational transformation](https://github.com/Aaaaash/blog/issues/10)  操作转换，类似于google doc多人编辑时
- Quadtree / Rtree ☑️
  - 四叉树是一种数据结构，每一个节点有四个孩子。一般需要用到四叉树的情况往往是二位平面，通过把区域分成四个区块来定义。
  - 应用：比如确定并显示一条曲线的具体位置时
- Ray casting ☑️
- Reverse index ✅
- Rsync algorithm ✅
- Trie algorithm ✅

### Bloom filter 

Bloom Filter 是由一个长度为 m 的比特位数组（bit array）与 k 个哈希函数（hash function）组成的数据结构。位数组均初始化为 0，所有哈希函数都可以分别把输入数据尽量均匀地散列。

当要插入一个元素时，将其数据分别输入 k 个哈希函数，产生 k 个哈希值。以哈希值作为位数组中的下标，将所有 k 个对应的比特置为 1。

当要查询（即判断是否存在）一个元素时，同样将其数据输入哈希函数，然后检查对应的 k 个比特。如果有任意一个比特为 0，表明该元素一定不在集合中。如果所有比特均为 1，表明该元素有（较大的）可能性在集合中。为什么不是一定在集合中呢？因为一个比特被置为 1 有可能会受到其他元素的影响，这就是所谓“假阳性”（false positive）。相对地，“假阴性”（false negative）在 Bloom Filter 中是绝不会出现的。

**优点：**

- 不需要存储数据本身，只用比特表示，因此空间占用相对于传统方式有巨大的优势，并且能够保密数据；
- 时间效率也较高，插入和查询的时间复杂度均为O(k)；
- 哈希函数之间相互独立，可以在硬件指令层面并行计算。

**缺点：**

- 存在假阳性的概率，不适用于任何要求 100% 准确率的场景；
- 只能插入和查询元素，不能删除元素，这与产生假阳性的原因是相同的。我们可以简单地想到通过计数（即将一个比特扩展为计数值）来记录元素数，但仍然无法保证删除的元素一定在集合中。

所以，Bloom Filter 在对查准度要求没有那么苛刻，而对时间、空间效率要求较高的场合非常合适，本文第一句话提到的用途即属于此类。另外，由于它不存在 **假阴性** 问题，所以用作“不存在”逻辑的处理时有奇效，比如可以用来作为 **缓存系统（如Redis）的缓冲，防止缓存穿透**。

### Lossy Counting Method

算法本身的作用是**找出长度为 N 的数据流中出现频率超过 s % 的元素，保证误差小于 a %。**其中 s 与 a 是传入的参数，a 一般设定为 s 的十分之一。此算法从数学上保证：

1. 在[数据流](https://www.zhihu.com/search?q=数据流&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A1030890806})中，出现频率高于 s * N 的元素最后都会输出。
2. 在数据流中，如果出现频率低于 ( s - a ) * N 的元素不会被输出。
3. 估算的出现次数与实际次数的差距不会高于 a * N。

算法实现起来也相对简单，只有三步：

1. 把现有的数据分成 N 个窗口，每个窗口的大小为 1 / a。
2. 按顺序统计每一个窗口里面元素出现的个数并减去 1。
3. 把剩下的值加起来并且返回所有大于 (s-a) * N 的值。

优点：

- 容易实现以及修改，能够控制元素频率以及误差。
- 快速高效，建立哈希表 V 只需要遍历一遍数据即可。
- 这个算法也适用于“如何设计一个显示最热门的 100个 标签的系统“这类场景。

缺点：

- 这个算法适用于玩家分布比较集中的情况，也就是说有很多玩家的分数是相同的。如果玩家的分数都不同，或者分布零散的话，哈希表 V 需要内存维护大量元素，会退化成桶排序。
- FindRank需要在哈希表 V 中遍历所有比该分数大的桶。


### Frugal Streaming 

### Geohash / S2 Geometry  地理位置

### Leaky bucket / Token bucket  限流器

Lossy Counting ，tok频繁 hashmap不停轮换请counter为0的

[Operational transformation](https://github.com/Aaaaash/blog/issues/10)  操作转换，类似于google doc多人编辑时

Quadtree / Rtree ☑️

  - 四叉树是一种数据结构，每一个节点有四个孩子。一般需要用到四叉树的情况往往是二位平面，通过把区域分成四个区块来定义。
  - 应用：比如确定并显示一条曲线的具体位置时

- Ray casting ☑️

- Reverse index ✅

- Rsync algorithm ✅

- Trie algorithm ✅

## 常见题目

> [系统设计参考1](https://soulmachine.gitbooks.io/system-design/content/cn/tinyurl.html) Done
>
> [系统设计参考2](https://wizardforcel.gitbooks.io/system-design-primer/content/solutions/system_design/pastebin/) 差不多，图片都失效了。

### 分布式ID生成器

发号器产生的 ID 一般都是 64 位整数，这样对数据库比较友好，容量也能满足业务需求





核心需求

- 全局唯一(unique)
- 按照时间粗略有序(sortable by time)
- 尽可能短

雪花算法: 

- 4个字节表示的Unix timestamp,
- 3个字节表示的机器的ID
- 2个字节表示的进程ID
- 3个字节表示的计数器

时间在前面可保证有序



使用mysql自增id, 比如8台机器，每个机器每次+8。

不用进程ID就可以保证单调递增，但依赖时间。 **雪花算法几乎可以是非常完美了，但它有一个致命的缺点 —— 强依赖机器时间。** 如果机器上的系统时间回拨，即时间较正常的时间慢，那么就可能会出现发号重复的情况。对于这种情况，我们可以在本地维护一个文件，写入上次的时间戳，随后与当前时间戳比较。如果当前时间戳小于上次时间戳，说明系统时间出了问题，应该及时处理。

- 比如 让程序等待一段时间（如等待到 `last_timestamp` 对应的时间后），保证生成的 ID 是单调递增的。
- 维护一个逻辑时间（`logical_timestamp`），如果检测到时间回拨，则使用 `last_timestamp + 1` 作为时间戳，以避免回拨带来的影响。



前面加namespace, appId这样区分，分业务。



- 时间戳可以变成HLC时间戳，它是物理时钟 + 逻辑时钟，同时定时ping pong取最大值，每个物理时钟的逻辑时钟重新加一。
- 提前缓存生成的id到本地内存。



其它实现: mysql自增主键， redis自增。

### 短网址系统

需求

- 足够短

网上链接大概45亿，长度7的字符串足够了，每个元素是大小写字母+数字

用分布式id生成器生成短链，用Nosql kv来存， 短网址-> 长网址

使用302临时重定向。

id生成器是数字，再变成62进制这样变成字符串。



### 信息流

[专门blog](https://mafulong.github.io/2020/12/28/Feed%E6%B5%81/)

### 定时任务调度器

[参考个人blog](https://mafulong.github.io/2021/04/06/%E5%AE%9A%E6%97%B6%E5%99%A8/)

### API限速

[参考个人blog: 限流器](https://mafulong.github.io/2021/04/12/%E9%99%90%E6%B5%81%E5%99%A8/)



### 设计线程安全的hashmap

hashmap可以拉链法，也可以java8一样，不是拉链，而是个红黑树，解决了hash冲突后的问题。

可以像concurrentHashMap一样采用分段锁保护。

### 实时输出最近一个小时内访问频率最高的10个IP

1. 3600s，3600个hashmap，存储每一秒的每个ip的访问次数。

2. 同时还要新建一个固定大小为10的小根堆，用于存放当前出现次数最大的10个IP。堆顶是10个IP里频率最小的IP。



3. 每次来一个请求，就把该秒对应的HashMap里对应的IP计数器增1，并查询该IP是否已经在堆中存在，

- 如果不存在，则把该IP在3600个HashMap的计数器加起来，与堆顶IP的出现次数进行比较，如果大于堆顶元素，则替换掉堆顶元素，如果小于，则什么也不做
- 如果已经存在，则把堆中该IP的计数器也增1，并调整堆

4. 需要有一个后台常驻线程，每过一秒，把最旧的那个HashMap销毁，并为当前这一秒新建一个HashMap，这样维持一个一小时的窗口。

5. 每次查询top 10的IP地址时，把堆里10个IP地址返回来即可。

如果内部存不下，可以使用redis等，同时时间跨度长的话还可以桶塌缩，近似统计。

### 设计一个负载均衡

负载均衡策略需要将请求均匀分配到各个服务节点，避免出现请求集中在某一点上的情况。有时会考虑节点权重，会话粘连等需求

> [参考](https://juejin.cn/post/6844903648460292109)

服务端节点列表存储下来，定时探活。

- 随机：数组存储列表，随机数字。

- 无权重轮询：移动cursor，数组

- 有权重轮询：移动cursor，数组。**权重分别为1,2,3的3该节点A,B,C，会先请求A一次，再请求B两次，再请求C三次**

- 有权重的平滑权重轮询

  - 所谓平滑, 即在一定的时间内, 不仅服务器被选择的次数分布和权重一致，满足权重要求，且调度算法还能比较均匀的选择节点分配请求

  - totalWeight：保存所有节点的权重和，该值在后续流程中保持不变

    nodeOriginWeight：保持每个节点的原始权重，在后续流程中也保持不变

    nodeCurWeights：保存每个节点的当前权重，该数组在后续每次计算请求应该分配到哪个节点时都会发生变化，初始化为每个节点的权重

    每次选择节点，都会执行以下3步

    - 选出当前权重中，值最大节点a
    - 将a的当前权重值减去**totalWeight**
    - 将每个当前权重加上每个节点的原始权重

  - 原理：**若某个节点增长越快，则越有概率被选中，而增长的速度和权重大小成正比，因此节点权重越大，越有概率被选中。相反或节点权重越小，增长成为最大当前权重节点的速度越慢，被选中的概率较低，从而达到按权重分配请求的效果**

    当每个节点被选中后，减去的值都相等，由于减去了一个较大的值（所有节点原始权重总和），**使得该节点在下几次请求中，被选中的概率较低，因为恢复成为最大值需要时间。从而达到平滑的效果**

- 最小活跃数：按活跃数排序，可以是avl树、跳表等。每次请求+1

- 一致性hash：参考个人blog，如数组二分查找、红黑树等，虚拟节点，hash个数字。



### 配置中心



配置中心的核心作用是 **存储和管理系统的配置信息**，并 **确保所有服务获取到的配置数据是正确的**。如果配置数据不一致，可能会导致 **系统故障、服务异常、甚至宕机**，因此 **一致性比可用性更重要**。 如果选择 AP，可能在分区期间读取到旧数据，导致配置回滚或者错误行为。



配置信息存储之后，需要考虑如何将配置的变更推送给服务端，这样就可以实现配置的动态变更，也就是说不需要重启服务器就能让配置生效了。

一般会有两种思路来实现变更推送：一种是轮询查询的方式；一种是长连推送的方式。

比较是否变化，可以比较MD5值，来降低全部拉取的带宽压力。 由于配置中心里存储的配置项变化的几率不大，所以使用这种方式后，每次轮询请求就只是返回一个MD5值，可以大大地减少配置中心服务器的带宽。



另一种长连的方式，它的逻辑是在配置中心服务端保存每个连接关注的配置项列表。这样，当配置中心感知到配置变化后，就可以通过这个连接，把变更的配置推送给客户端。这种方式需要保持长连，也需要保存连接和配置的对应关系，实现上要比轮询的方式复杂一些，但是相比轮询方式来说，能够更加实时地获取配置变更的消息。

配置服务中存储的配置变更频率不高，所以对于实时性要求不高，但是期望实现上能够足够简单，所以如果选择自研配置中心的话，可以考虑使用轮询的方式。



可用性：

- 配置存储是分级的，有公共配置，有个性的配置，一般个性配置会覆盖公共配置，这样可以减少存储配置项的数量；
- 配置中心可以提供配置变更通知的功能，可以实现配置的热更新；
- 配置中心关注的性能指标中，可用性的优先级是高于性能的，一般我们会要求配置中心的可用性达到99.999%，甚至会是99.9999%。
- 在配置中心的客户端上，增加两级缓存：第一级缓存是内存的缓存；另外一级缓存是文件的缓存。



### Metrics 监控系统

[参考](https://www.cnblogs.com/myshowtime/p/16445208.html)

理论下，常规数据库可以支持时间序列数据， 但是需要数据库专家级别的调优后，才能满足数据量比较大的场景需求。

具体点说，关系型数据库没有对时间序列数据进行优化，有以下几点原因

- 在滚动时间窗口中计算平均值，需要编写复杂且难以阅读的 SQL。
- 为了支持标签（tag/label）数据，我们需要给每个标签加一个索引。
- 相比之下，关系型数据库在持续的高并发写入操作时表现不佳。



相比之下，专门对时间序列数据优化的时序数据库，更适合这种场景。

OpenTSDB 是一个分布式时序数据库，但由于它基于 Hadoop 和 HBase，运行 Hadoop/HBase 集群也会带来复杂性。Twitter 使用了 MetricsDB 时序数据库存储指标数据，而亚马逊提供了 Timestream 时序数据库服务。

根据 DB-engines 的报告，两个最流行的时序数据库是 InfluxDB 和 Prometheus ，它们可以存储大量时序数据，并支持快速地对这些数据进行实时分析。

如下图所示，8 核 CPU 和 32 GB RAM 的 InfluxDB 每秒可以处理超过 250,000 次写入。



InfluxDB 使用的针对于时序数据的 Flux 查询语言会更简单更好理解，如下

```css
from(db:"telegraf")
  |> range(start:-1h)
  |> filter(fn: (r) => r._measurement == "foo")
  |> exponentialMovingAverage(size:-10s)
```



**Tag** 主要用于对数据进行 **过滤** 和 **分组** 操作。由于 **Tag** 是经常被用来做查询条件的字段，所以它们通常会被索引以提高查询效率。

**Field**（字段）则是实际存储数据的部分，它是 **Tag** 的对立面，**Field** 不是索引的，查询时需要扫描整个数据表。



‌**InfluxDB的读音是['ɪnflʌks]**‌

- **高效写入**：InfluxDB 采用了专门的存储引擎优化数据的写入速度。它使用了 **Log-Structured Merge Tree (LSM-Tree)** 存储引擎，确保在大量并发写入时，能够保持良好的写入性能。
  - 在 InfluxDB 的 LSM-Tree 实现中，**key** 由 `Measurement`、`Tag` 和 `Time` 组成，用来唯一标识每个时间序列数据点。**value** 存储的是 `field` 数据，即实际的测量值（如温度、湿度等）。通过时间戳和有序存储，InfluxDB 能够高效地执行时间范围查询，同时利用 MemTable 和 SSTables 来优化写入和查询性能。合并（Compaction）操作通过删除重复数据、压缩存储和优化磁盘空间来确保系统的高效运行。
- **时间序列数据压缩**：InfluxDB 在存储时会对数据进行压缩，尤其是对时间序列数据中相似数据的压缩。这大大减少了存储空间的需求。它采用了 **TSSD (Time-Structured Merge Tree)** 结构进行存储，支持高效的数据压缩和合并。
  - TSSD 使用的是基于时间的分层合并策略，这与传统的 LSM-Tree 合并策略有所不同。具体来说，TSSD 会根据时间顺序将数据分层存储，每个层包含一定时间范围内的数据。例如，第一层存储最新的数据，第二层存储稍早的数据，依此类推。合并过程中，InfluxDB 会选择具有相同时间范围的 SSTables 进行合并。
  - 压缩，比如差分编码：利用相邻数据点之间的差值进行编码，而不是直接存储每个数据点的完整值。这样可以大大减少存储空间需求，特别是在数据变化较小时



InfluxDB 对 **Tag** 的每个键（key）都创建一个倒排索引，该索引的结构如下：

- **Key**：Tag 键名（例如 `location`）。
- **Value**：包含该键的所有数据点的 ID 或时间戳。每个包含该 Tag 的数据点都会在该倒排索引中列出。

例如，如果你有多个数据点都包含 `location=us-east-1`，倒排索引会记录哪些数据点的 `location` 是 `us-east-1`。这样，当你执行查询时，InfluxDB 可以快速根据索引找到所有符合条件的记录。

- **索引存储在内存中**：为了提高查询性能，InfluxDB 会将 **Tag** 索引的数据存储在内存中（使用 LSM-Tree 和类似 Redis 的数据结构），并定期将其持久化到磁盘。
- **支持多值索引**：每个 Tag 可以具有多个不同的值，倒排索引会记录每个值对应的时间序列数据点。



倒排索引的存储通常使用高效的 **B+ 树** 或 **跳表**（Skip List）等数据结构。通过这些数据结构，倒排索引能够支持快速的查找、插入和删除操作。

- **B+ 树**：对于磁盘上的数据，倒排索引通常会存储在 **B+ 树** 结构中，这样可以利用 B+ 树的有序性实现高效的范围查询（如时间区间查询）。
- **跳表（Skip List）**：在内存中的倒排索引结构可能使用 **跳表**，它支持快速的查询和范围查找。



Flink 适用于任何需要处理实时、大规模数据流的应用，特别是在对低延迟和高吞吐量要求较高的场景下。常见的应用领域包括金融、互联网、物联网、智能城市、企业监控等。





### ELK海量日志收集

[参考](https://juejin.cn/post/7001801178740686878)



ELK 是三个开源软件的缩写，分别表示：Elasticsearch、Logstash、Kibana，其中还没有提到轻量级的日志收集工具 Filebeat。以下是这几个组件的简介：

- **Filebeat**，轻量级的日志收集处理工具
- **Logstash**，主要是用来日志的搜集、分析、过滤日志的工具，支持大量的数据获取方式。一般工作方式为 c/s 架构，client 端安装在需要收集日志的主机上（这里用 Filebeat 代替），server 端负责将收到的各节点日志进行过滤、修改等操作存储到 ES 或者其他存储系统上。
- **Elasticsearch**，是个开源分布式搜索引擎，提供搜集、分析、存储数据三大功能。
- **Kibana**，提供的日志分析友好的 Web 界面，可以帮助汇总、分析和搜索重要数据日志。



### 设计Key-Value存储引擎

[参考个人blog: leveldb设计及实现](https://mafulong.github.io/2021/01/11/leveldb%E5%92%8CRocksDB/)



### 网络爬虫

[参考个人blog: 搜索引擎](https://mafulong.github.io/2021/01/01/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/)



### 分布式PageRank

[参考个人blog: 搜索引擎](https://mafulong.github.io/2021/01/01/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/)



### 大数据

[参考个人blog算法系列大数据问题](https://mafulong.github.io/2018/07/11/%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%97%AE%E9%A2%98/)





### 排行榜

[参考个人博客](https://mafulong.github.io/2022/01/24/%E6%8E%92%E8%A1%8C%E6%A6%9C/)





